# Mini ChatGPT for PDFs (Streamlit MVP)

**Upload â†’ Extract â†’ Chunk â†’ Embed â†’ Retrieve â†’ Answer**  
Answers come **only** from your PDFs. If the info isnâ€™t in the docs, the app replies **â€œI donâ€™t know.â€**

---

## âœ¨ Features
- **Multi-PDF upload** (text-based PDFs)
- **Text extraction** with PyMuPDF
- **Token-aware chunking** (configurable size & overlap)
- **Embeddings**: OpenAI `text-embedding-3-small`
- **Vector store**: Chroma (persistent on disk)
- **Top-k retrieval** + **GPT-3.5** for grounded answers
- **Source controls**: filter search by specific file(s)
- **Index tools**: per-file delete, â€œRebuild index from ALL extracted filesâ€
- **De-dup by content**: same file re-uploads are skipped
- **Index summary**: shows chunk counts per file

---

## ğŸš€ Quickstart

### Requirements
- Python **3.10+**
- An OpenAI API key

### Setup
```bash
git clone https://github.com/itsjahaziel/mini-chatgpt-pdf.git
cd mini-chatgpt-pdf
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

cp .env.example .env
# edit .env and set:
# OPENAI_API_KEY=sk-...your key...

streamlit run app.py

ğŸ§­ Usage
	1.	Upload & Extract
Upload one or more text-based PDFs. The app extracts text and shows a preview.
	2.	Build / Manage Index
	â€¢	Select the files to index and click Create / Update index
	â€¢	Or click Rebuild index from ALL extracted files
	â€¢	Use Delete embeddings for a file if you need to remove a single document from the index
	3.	Ask Questions
	â€¢	Optionally limit search to specific file(s)
	â€¢	Ask your question â†’ the app retrieves top-k chunks and answers using only that context
	â€¢	If itâ€™s not in the docs, it responds â€œI donâ€™t know.â€

Tip: Ask specific questions for best results (dates, roles, definitions, etc.).

â¸»

ğŸ§© How it works (RAG pipeline)
	1.	Extract text from PDFs (PyMuPDF)
	2.	Chunk text into token-bounded pieces (size & overlap sliders)
	3.	Embed chunks with OpenAI text-embedding-3-small
	4.	Store vectors in Chroma (persistent at data/processed/chroma/)
	5.	Retrieve top-k similar chunks for a query
	6.	Answer with gpt-3.5-turbo using a strict prompt: use only the provided context; otherwise say â€œI donâ€™t know.â€

â¸»

ğŸ”§ Configuration
	â€¢	.env
OPENAI_API_KEY=sk-your-key-here

	â€¢	In the app sidebar:
	â€¢	Tokens per chunk, Token overlap
	â€¢	Top-k retrieved chunks
	â€¢	LLM model (default: gpt-3.5-turbo)

â¸»

ğŸ“ Project structure

mini-chatgpt-pdf/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ pdf_loader.py      # PyMuPDF extraction
â”‚   â”œâ”€â”€ chunker.py         # token-aware chunking (tiktoken)
â”‚   â”œâ”€â”€ embedder.py        # OpenAI embeddings + Chroma (persistent)
â”‚   â”œâ”€â”€ retriever.py       # (kept minimal or unused in latest)
â”‚   â””â”€â”€ llm.py             # GPT-3.5 answer w/ strict context-only prompt
â””â”€â”€ data/
    â”œâ”€â”€ uploads/           # saved PDFs (ignored by git)
    â””â”€â”€ processed/chroma/  # vector DB (ignored by git)


ğŸ›¡ï¸ Security & privacy
	â€¢	Never commit your real .env. This repo ignores it by default.
	â€¢	PDFs are stored locally (data/uploads/) and embeddings locally (data/processed/chroma/).
	â€¢	This is an MVP for demosâ€”review before using on sensitive data.

â¸»

ğŸ§¹ Troubleshooting
	â€¢	â€œI uploaded the same file and it duplicatesâ€
The app de-dups by content hash; same file bytes wonâ€™t be re-saved/extracted. Old manual copies can be removed from data/uploads/.
	â€¢	Chroma error like no such table: databases
The app auto-repairs. If needed, manually clear and reindex:

rm -rf data/processed/chroma


